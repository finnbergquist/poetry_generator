# AI Poetry Generator App by Finn Bergquist

### Description

This intelligent poetry generator uses a five-layer recurrent neural network,
trained on over 17,000 lines of poetry from a large assortment of famous poets.
The network in this repository pretrained, but it can be trained differently as
with other hyperparameters. (see How to Run Program section)
The neural network predicts the sequential order of words, allowing for
a user to ask for the next word when generating poetry. The frontend of this
project utilizes that capability, combining the autonomous way to add a next
word with a manual way as well. The user could theoretically write a few words,
then ask the AI for its predictions for the following word and then respond
back to the AI. Noise was added for the next word selection process to ensure
that similar results were not repeated due to the deterministic nature of the
pretrained network.

The network is trained, loaded, and queried from the poetry_agent class which
is implemented in the Flask app backend. The frontend is written in html/css,
sending backend calls for each different functionality of the app(ex. store the
poetry information, evaluate saved poems, speak poems, ask for a new AI word,
etc.)

The saved poetry can be viewed in the Recorded Poems tab, where they can be
audibly read aloud. Additionally, this is where the evaluative RMSE
(root mean squared error) metric can be seen. The purpose of this metric is to
show how far the generated poetry deviates from the deterministic policy of
the neural network. If the highest rated output word was always chosen, the
RMSE score would be zero. For example, say the seed text is:

```
This is ___
```

The neural network output might look like:

```
[0.234, 0.546, 0.456]
```

corresponding to:

```
[cheese, sparta, fun]
```

With added noise, the system might still select the word "fun", and then this
words contribution to the RMSE would be

```
(y'-y)^2 = (0.546 - 0.456)^2
```

where y = the selected word and y'= the argmax word

### How to Run the Program

### Install Dependencies

* numpy
* keras
* tensorflow
* sklearn
* flask

For each of these, run:
```
pip install dependency_name
```

### Start a local server running the app:
```
flask run
```
This will start a local development server with a link in the terminal where the web app can be accessed

### Re-training the Model:

Adjust the hyperparameters in the poetry_agent __init__() method however you like and run
```
pip train.py
```


# Challenges

Working on this system challenged my ability to create a machine learning agent that functions within a web app tech stack. 
I found that the best way to do this is to pre-train the model, and load up the pre-trained model information when launching
the web app. I have very little experience building a front-end as well. While this front-end is nothing too fancy, it 
accomplishes exactly what I wanted. I am happy with the user interactive elements implemented in the UI which allow the user
to add their own words, save the poems, and go back to look at the saved poems.

A challenge that I did not have quite as much success with was finding a way to ensure that the AI would suggest coherent 
words without repeating itself too much. I spent a lot of time tuning the hyperparameters but never achieved an optimal balance.

A final challenge that I tackled in this project was adding elements of randomness to the deterministic nature of the pre-trained
poetry agent. In the end, I found a good way to do this was sorting the words based on their outputs from the neural network and
randomly selecting a word from the top 10. This avoided a problem I had where the system would generate cycles of words resulting
in lots of repeated phrases.

### Inspirations

## Does human–AI collaboration lead to more creative art? Aesthetic evaluation of human-made and AI-generated haiku poetry by Jimpei Hitsuwaria, Yoshiyuki Ueda, Woojin Yuna, and Michio Nomuraa

https://www.sciencedirect.com/science/article/pii/S0747563222003223

This article explains a study that they did where they created poems using three
different strategies. The first was purely generated by AI, the second was purely
generated by human poets, and the third involved an AI poem generator with human intervention.
From human evaluations of the poems, scoring them based on their "beauty", the
AI and human collaborations scored the highest! This inspired my choice to add
a button and text box to allow the user to add their own poetic choices to the
AI generated poems. I believe the concept of human-AI collaboration holds enormous
potential which is not just limited to art.

## Poet: Automatic Poetry Composition through Recurrent Neural Networks with Iterative Polishing Schema by Rui Yan, Department of Computer Science, Peking University

https://dl.acm.org/doi/10.5555/3060832.3060934

This paper outlines an approach to using a recurrent neural network to create a
generative poetry system. I was drawn to their evaluative strategy. One type
of evaluation is a human evaluation, where they asked real people about a
poem's fluency, poetic-ness, coherence, and meaning. The author of the article
explains how important the human evaluation was for creating a human-like
intelligent system. Another evaluative metric that the author implemented was
Perplexity(PPL). Perplexity is an entropy based evaluative calculation. It
measures how accurate the model is, given the underlying probability
distribution in the data set. A lower perplexity score indicates a model that
has very accurate probabilistic predictions. Usually, lower perplexity is
associated with better AI poetry; however, it could also represent a lack of
creative risk taking by the intelligent agent. The application of entropy based
calculations inspired me to use categorical cross-entropy as the loss function
for my neural network’s back propagation.

Additionally, this article also made me think about creating a way to evaluate
how well my system's predictions are relative to the probabilities that it
already knows. I wanted to this would be a useful metric to use after
the system was already trained. Without any noise added, the system was
deterministic. I wanted to observe how the RMSE between the deterministic word
and the predicted word with noise differed as more noise was added. I predicted
that there would be a sweet spot where the system would avoid repeating exact
phrases from the training data while also create coherent word combinations.
RMSE calculation can be observed in the Saved Poem page.

## Human Competence in Creativity Evaluation by Carolyn Lamb, Daniel G. Brown, Charles L.A. Clarke

https://axon.cs.byu.edu/ICCC2015proceedings/5.2Lamb.pdf

This paper's core debate is the balancing of two different high-level methods
for evaluating creativity in computational systems. The first is the consensual
assessment technique: asking humans their opinion about the creative value of an
output. The second method is calculating scores for novelty, value, skill, etc.
based on empirical information from the output. The authors conducted an
experiment where they gathered poets of varying ability and experience, asked
them to write poems and evaluated them using both methods. The experienced
poets scored much higher on all the objective creativity metrics. The consensual
assessment technique was applied using non-expert judges, and interestingly,
the judges rated the poems written by amateurs more highly. The experiment
proved that non-expert judges could not make accurate guesses for the creativity
metrics. Ultimately, the judges conclude that it is challenging to evaluate
creativity using the consensual assessment technique because it requires
expert judges to get valuable results. At the same time, the empirical metrics
lack the human aspect that is so important to creativity, so for a truly robust
evaluation, neither of these techniques is sufficient by themselves.
