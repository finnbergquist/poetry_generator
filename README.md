# poetry_generator

# Description

This intelligent poetry generator uses a five layer recurrent neural network,
trained on over 17,000 lines of poetry from a large assortment of famous poets.
The network in this reposatory pretrained, but it can be trained differently as
with other hyperparameters. (see how to run program section)
The neural network predicts the sequential order of words, allowing for
a user to ask for the next word when generating poetry. The frontend of this
project utilizes that capability, combining the autonomous way to add a next
word with a manual way as well. The user could theoretically write a few words,
then ask the AI for its predictions for the following wordd and then respond
back to the AI. Noise was added for the next word selection process to ensure
that similair results were not repeated due to the detirministic nature of the
pretrained network.

The network is trained, loaded, and queried from the poetry_agent class which
is implemented in the Flask app backend. The frontend is written in html/css,
sending backend calls for each different functionality of the app(ex. store the
poetry information, evaluate saved poems, speak poems, ask for a new AI word,
etc.) The saved poetry can be viewed in the Recorded Poems tab, where they can be
audibly read aloud. 

# Evaluation

Additionally, in the recorded poems page, the evaluative RMSE
(root mean squared error) metric can be seen. The purpose of this metric is to
show how far the generated poetry deviates from the deterministic policy of
the neural network. If the highest rated output word was always chosen, the
RMSE score would be zero. For example, say ourn seed text is:

```
This is ___
```

The neural network output might look like:

```
[0.234, 0.546, 0.456]
```

corresponding to:

```
[cheese, sparta, fun]
```

With added noise, the system might still select the word "fun", and then this
words contribution to the RMSE would be

<img width="554" alt="Screen Shot 2022-11-22 at 1 34 52 PM" src="https://user-images.githubusercontent.com/61434761/203394281-8d053952-c046-41a3-8c38-a4fd43161576.png">

This would be one of the additive terms in the total root mean squared error:

<img width="330" alt="Screen Shot 2022-11-22 at 1 33 23 PM" src="https://user-images.githubusercontent.com/61434761/203393943-3598b626-73da-4ce5-8661-27ac6712e42d.png">

where y = the selected word and y'= the argmax word

Ultimately, a high RMSE score reflects a poem that selected words further from the argmax. These types
of poems are more novel but generally not as novel. Low RMSE scores reflect poems that took fewer
risks and stuck to the highest weighted word suggestions.

# How to Run the Program

Install Dependencies

* numpy
* keras
* tensorflow
* sklearn
* flask

For each of these, run:

```
pip install dependency_name
```

Start a local server running the app:
```
flask run
```
This will start a local development server with a link in the terminal where the web app can be accessed

# Challenges

# Inspirations

### Does humanâ€“AI collaboration lead to more creative art? Aesthetic evaluation of human-made and AI-generated haiku poetry by Jimpei Hitsuwaria, Yoshiyuki Ueda, Woojin Yuna, and Michio Nomuraa

https://www.sciencedirect.com/science/article/pii/S0747563222003223

This article explains a study that they did where they created poems using three
different strategies. The first was purely generated by AI, the second was purely
generated by human poets, and the third involved an AI poem generator with human intervention.
From human evaluations of the poems, scoring them based on their "beauty", the
AI and human collaborations scored the highest! This inspired my choice to add
a button and text box to allow the user to add their own poetic choices to the
AI generated poems. I believe the concept of human-AI collaboration holds enormous
potential which is not just limited to art.

### Poet: Automatic Poetry Composition through Recurrent Neural Networks with Iterative Polishing Schema by Rui Yan, Department of Computer Science, Peking University

https://dl.acm.org/doi/10.5555/3060832.3060934

This paper outlines an approach to using a recurrent neural newtork to create a
generative poetry system. I was drawn to their evaluative strategy. One type
of evaluation is a human evaluation, where they asked realn people about a
poem's fluency, poeticness, coherence, and meaning. The author of the article
explains how important the human evaluation was for creating a human-like
intelligent system. Another evaluative metric that the author implemented was
Perpkexity(PPL). Perplexity is an entropy based evaluative calculation. It
measures how accurate the model is, given the underlying probability
distribution in the data set. A lower perplexity score indicates a model that
has very accurate probablistic predictions. Usually, lower perplexity is
associated with better AI poetry; however, it could also represent a lack of
creative risk taking by the inteligent agent. The application of entropy based
calculations inspired me to use categorical cross-entropy as the loss function
for my neural netweork's back propogation. 

Additionally, this article also made me think about creating a way to evaluate
how well my system's predictions are relative to the probabilities that it
already knows. I wanted wanted to this would be a useful metric to use after
the system was already trained. Without any noise added, the system was
deterministic. I wanted to observe how the RMSE between the deterministic word
and the predicted word with noise differed as more noise was added. I predicted
that there would be a sweet spot where the system would avoid repeating exact
phrases from the training data while also create coherent word combinations.
RMSE calculation can be observed in the Saved Poem page. This strategy was
a simpler way to calculatea metric that covers the general idea of the perplexity
calculation.

### Human Competence in Creativity Evaluation by Carolyn Lamb, Daniel G. Brown, Charles L.A. Clarke

https://axon.cs.byu.edu/ICCC2015proceedings/5.2Lamb.pdf

This paper's core debate is the balancing of two different high level methods
for evaluating creativity in computational systems. The first is the consensual
assesment technique: asking humans their opinion about the creative value of an
output. The second method is calculating scores for novelty, value, skill, etc.
based on empirical information from the output. The authors conducted an
experiment where they gathered poets of varying ability and experience, asked
them to write poems, and evaluated them using both methods. The experienced
poets scored much higher on all the objective creativity metrics. The consensual
assessment techniqe was applied using non-expert judges, and interestingly,
the judges rated the poems written by amateurs more highly. The experiment
proved that non-expert judges could not make accurate guesses for the creativity
metrics. Ultimately, the judges conclude that it is challenging to evaluate
creativity using the consensual assessment technique because it requires
expert judges to get valuable results. At the same time, the empirical metrics
lack the human aspect that is so important to creativity, so for a truly robust
evaluation, neither of these techniques is sufficient by themselves.
